{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"data_oneleak_train(stratify)_without_outliers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_points = utils.read_data(path=f\"./data/{filename}.csv\")\n",
    "df_vacuumports = utils.read_data(path=\"./data/vacuumports.csv\")\n",
    "df_hyperplanes = utils.read_data(path=\"./data/hyperplanes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers = [15,38,208,218,293]\n",
    "# df_points = df_points.drop(index=outliers).reset_index().drop([\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df_points.drop([\"y1\",\"y2\"],axis=1), df_points[[\"y1\",\"y2\"]]\n",
    "W, b = tf.constant(df_hyperplanes[[\"w1\",\"w2\"]], dtype=tf.float32), tf.Variable(df_hyperplanes[\"b\"], dtype=tf.float32)\n",
    "theta = b\n",
    "# b is tf.Variable because we need to optimize its value, tf.constant doesn't allow the value of its tensor to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voronoi_regions_normals, voronoi_regions_offsets = utils.create_voronoi_regions_normals_and_offsets(df_hyperplanes,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_{j,k} \\in \\{-1, 0, 1\\}, \\\\\n",
    "s_k = \\begin{pmatrix}\n",
    "s_{1,k} \\\\\n",
    "s_{2,k} \\\\\n",
    "\\vdots \\\\\n",
    "s_{j,k} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "\n",
    "s_{j,k}= \\begin{cases}\n",
    "0 & \\text{if } j \\notin I_k \\\\\n",
    "1 & \\text{if point lies within the region} \\\\\n",
    "-1 & \\text{if point lies outside of the region}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s_k(region):\n",
    "    \"\"\"\n",
    "    Generates s_k for all the hyperplanes.\n",
    "    Consists of only 3 elements {-1,0,1}\n",
    "    if point lies on the correct side of hyperplane\n",
    "        then s_j_k = 1\n",
    "    if point lies on the wrong side of the hyperplane\n",
    "        then s_j_k = -1\n",
    "    if hyperplane is not related to the current voronoi region\n",
    "        then s_j_k = 0\n",
    "    \"\"\"\n",
    "    df_region_hyperplanes = df_hyperplanes[df_hyperplanes[\"region\"]==int(region)]\n",
    "    s_k = np.zeros(df_hyperplanes.shape[0])\n",
    "    for index, row in df_hyperplanes.iterrows():\n",
    "        if row[\"region\"] == region:\n",
    "            s_k[index] = 1\n",
    "        else:\n",
    "            W, b = tf.constant([row[\"w1\"], row[\"w2\"]], dtype=tf.float32), tf.constant(row[\"b\"], dtype=tf.float32)\n",
    "            for _, region_hyperplane in df_region_hyperplanes.iterrows():\n",
    "                if utils.same_hyperplane(W,b,region_hyperplane[\"w1\"],region_hyperplane[\"w2\"],region_hyperplane[\"b\"]):\n",
    "                    s_k[index] = -1\n",
    "                    break        \n",
    "    return tf.constant(s_k, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s^{(i)} := \\underset{x_k=1, \\ldots, d}{\\arg \\max} \\ k \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\theta} \\ \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{1}^T \\text{ReLU}\\left(s^{(i)} \\circ \\left[ \\mathbf{W}y^{(i)} + \\mathbf{b} \\right]\\right) \\\\\n",
    "\\text{Where } \\mathbf{1} = (1, 1, 1, \\ldots)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a dictionary s_k for storing the value of s_k for each region. This will help us in minimizing the overall time as it will prevent calculating s_k for different region even tho they were calculated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_k_dict = dict()\n",
    "for i in range(len(df_vacuumports.index)):\n",
    "    s_k_dict[i+1] = generate_s_k(region=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cost_function(theta,df_points):\n",
    "    \"\"\"\n",
    "    Calculates the cost function which needs to be minimised.\n",
    "    cost = min_over_theta (1/m)sum_i=1_to_m ReLU(s_(i) o [W.y_(i) + b])\n",
    "    where, m := cardinality of input data.\n",
    "    \"\"\"\n",
    "    W = tf.constant(df_hyperplanes[[\"w1\",\"w2\"]], dtype=tf.float32)\n",
    "    point_coordinates = tf.constant(df_points[[\"y1\",\"y2\"]].values, dtype=tf.float32)\n",
    "    \n",
    "    vaccum_flows = df_points.iloc[:,:10].values\n",
    "\n",
    "    x_tilde_indices = tf.argmax(vaccum_flows, axis=1) + 1\n",
    "    s_k= tf.constant(np.array([s_k_dict[x] for x in x_tilde_indices.numpy()]))\n",
    "\n",
    "    # print(point_coordinates[:5],tf.transpose(W))\n",
    "    result = tf.matmul(point_coordinates, tf.transpose(W)) + theta\n",
    "    # print(f\"theta: {theta}, result -> theta: {result -  tf.matmul(point_coordinates, tf.transpose(W)) }\")\n",
    "    relu_result = tf.nn.relu(s_k * result)\n",
    "\n",
    "    # Sum up all problem costs to get the total cost\n",
    "    cost = tf.reduce_sum(relu_result)\n",
    "    return cost/len(df_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoronoiModel(tf.keras.Model):\n",
    "    def __init__(self,W,b):\n",
    "        super(VoronoiModel, self).__init__()\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x, y = inputs\n",
    "        ones = tf.ones(self.W.shape[0], dtype=tf.float32)\n",
    "        flow_region = tf.argmax(x) + 1      # feature extraction\n",
    "        s_k = tf.constant(generate_s_k(region=tf.constant(flow_region)), dtype=tf.float32)\n",
    "        result = tf.tensordot(self.W,y,axes=1) + self.b # W.y + b\n",
    "        relu_result = tf.tensordot(ones,tf.nn.relu(s_k * result), axes=1)\n",
    "        return relu_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cost = custom_cost_function(theta=theta,df_points=df_points)\n",
    "print(f\"The initial cost is {initial_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 1.01\n",
    "\n",
    "# result = pd.DataFrame(columns=[\"Epoch\",\"Learning rate\",\"Optimizer\",\"Step\",\"Accuracy\",\"Loss value\",\"Theta(b)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating an optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "optimizer_name = \"SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(x, y, df_points):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model = VoronoiModel(W=W,b=theta)\n",
    "        logits = model(inputs=(x,y))\n",
    "    grads = tape.gradient(logits,[theta])\n",
    "    optimizer.apply_gradients(zip(grads, [theta]))\n",
    "    loss_value = custom_cost_function(theta=theta, df_points=df_points)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "initial_accuracy = utils.calculate_accuracy(df_hyperplanes=df_hyperplanes,df_points=df_points,theta=theta)\n",
    "print(f\"Initial accuracy: {initial_accuracy}\")\n",
    "\n",
    "accuracy = initial_accuracy\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_train,y_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x=x_train,y=y_train,df_points=df_points)\n",
    "        accuracy = utils.calculate_accuracy(df_hyperplanes=df_hyperplanes,df_points=df_points,theta=theta)\n",
    "        result = pd.concat([result, pd.DataFrame([[epoch,learning_rate,\"SGD\",step,accuracy,loss_value.numpy(),[theta.numpy()]]], columns=result.columns)])\n",
    "    print(f\"Accuracy: {accuracy}, Training loss: {loss_value} -> for epoch {epoch}\")\n",
    "end_time = time.time()\n",
    "print(f\"Total time required: {end_time-start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f\"{epochs}_{learning_rate}_{optimizer_name}_{filename}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(f\"./Outputs/{output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = utils.calculate_accuracy(df_hyperplanes,df_points,theta)\n",
    "print(f\"The accuracy for training data is {round(training_accuracy*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cost_function(theta,df_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss value after {epochs} epoch: {loss_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"New theta: {theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperplanes[\"b\"] = theta.numpy()\n",
    "voronoi_regions_normals, voronoi_regions_offsets = utils.create_voronoi_regions_normals_and_offsets(df=df_hyperplanes,total_regions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marker_style(row):\n",
    "    if row[\"True region\"] == row[\"Predicted region\"]:\n",
    "        return \"o\"\n",
    "    else:\n",
    "        return \"x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_region = utils.true_regions(df=df_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_points[\"True region\"] = true_region\n",
    "df_points[\"Predicted region\"] = [utils.which_region(voronoi_regions_normals,voronoi_regions_offsets,np.array([row[\"y1\"], row[\"y2\"]]))  for _, row in df_points.iterrows()]\n",
    "df_points[\"Marker\"] = df_points.apply(marker_style, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_fill = np.array(np.loadtxt(\"./data/fill.csv\"))\n",
    "points_contour = np.array(np.loadtxt(\"./data/contour.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_points = 5000000\n",
    "y = np.concatenate([np.random.uniform(low=points_contour[:, 0].min(), high=points_contour[:, 0].max(), size=(1, n_test_points)),\n",
    "                    np.random.uniform(low=points_contour[:, 1].min(), high=points_contour[:, 1].max(), size=(1, n_test_points))], axis=0)\n",
    "\n",
    "y_regions = np.zeros(n_test_points)\n",
    "for index,point in enumerate(tf.transpose(y)):\n",
    "    y_regions[index] = utils.which_region(voronoi_regions_normals,voronoi_regions_offsets,np.array(point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "utils.plot_contour(points=points_contour,points_fill=points_fill)\n",
    "\n",
    "sns.scatterplot(df_points,x=\"y1\",y=\"y2\",hue=\"True region\",palette=\"colorblind\",s=30,style=\"Marker\", markers={\"x\": \"X\", \"o\": \"o\"},linewidth=1,zorder=1)\n",
    "\n",
    "df_test = pd.DataFrame(data=np.concatenate([y.T], axis=1), columns=['y1', 'y2'])\n",
    "df_test['region'] = y_regions\n",
    "sns.scatterplot(df_test, x='y1', y='y2', hue='region', palette='colorblind', s=10, linewidth=0, zorder=0, legend=False)\n",
    "\n",
    "plt.legend(title=\"Marker\", handles=[\n",
    "    plt.Line2D([], [], marker='X', linestyle='None', color='black', label='Point in wrong region'),\n",
    "    plt.Line2D([], [], marker='o', linestyle='None', color='black', label='Point in correct region')\n",
    "])\n",
    "plt.title(f\"Visualisation for Training data.\\nOptimizer = {optimizer_name}, Epochs = {epoch+1},\\nAccuracy = {round(training_accuracy*100,2)}, Loss = {round(float(loss_value.numpy()),2)}\\nwith outliers({filename}.csv)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"./plots/Final/Stratify/With outliers/Adaptive_voronoi_training_{filename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results.csv\", \"a\",encoding=\"UTF-8\") as csvfile:\n",
    "    csvwrite = csv.writer(csvfile)\n",
    "    csvwrite.writerow([epoch+1,learning_rate,optimizer_name,training_accuracy*100,loss_value.numpy(),[theta.numpy()],end_time-start_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Comparisons.csv\", \"a\",encoding=\"UTF-8\") as csvfile:\n",
    "    csvwrite = csv.writer(csvfile)\n",
    "    csvwrite.writerow([\"Adaptive Voronoi\",\"Train\",50,optimizer_name,learning_rate,training_accuracy*100,loss_value.numpy(),[theta.numpy()],f\"{filename}.csv\",True,end_time-start_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = f\"data_oneleak_test(stratify)_with_outliers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_points_test = pd.read_csv(f\"./data/{test_filename}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_region = utils.predicted_regions(voronoi_regions_normals=voronoi_regions_normals,voronoi_regions_offsets=voronoi_regions_offsets,df=df_points_test)\n",
    "testing_accuracy = utils.calculate_accuracy(df_hyperplanes,df_points=df_points_test,theta=theta)\n",
    "testing_loss = custom_cost_function(theta,df_points_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy on unseen data is {round(testing_accuracy*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The loss on unseen data is {testing_loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_points_test[\"True region\"] = utils.true_regions(df=df_points_test)\n",
    "df_points_test[\"Predicted region\"] = predicted_region\n",
    "df_points_test[\"Marker\"] = df_points_test.apply(marker_style, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "utils.plot_contour(points=points_contour,points_fill=points_fill)\n",
    "\n",
    "sns.scatterplot(df_points_test,x=\"y1\",y=\"y2\",hue=\"True region\",palette=\"colorblind\",s=30,style=\"Marker\", markers={\"x\": \"X\", \"o\": \"o\"},linewidth=1,zorder=1)\n",
    "\n",
    "df_test = pd.DataFrame(data=np.concatenate([y.T], axis=1), columns=['y1', 'y2'])\n",
    "df_test['region'] = y_regions\n",
    "sns.scatterplot(df_test, x='y1', y='y2', hue='region', palette='colorblind', s=10, linewidth=0, zorder=0, legend=False)\n",
    "\n",
    "plt.legend(title=\"Marker\", handles=[\n",
    "    plt.Line2D([], [], marker='X', linestyle='None', color='black', label='Point in wrong region'),\n",
    "    plt.Line2D([], [], marker='o', linestyle='None', color='black', label='Point in correct region')\n",
    "])\n",
    "plt.title(f\"Visualisation for Testing data.\\nOptimizer = {optimizer_name}, Epochs = {epoch+1},\\nAccuracy = {round(testing_accuracy*100,2)}, Loss = {round(float(testing_loss.numpy()),2)}\\nwith outliers({test_filename}.csv)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"./plots/Final/Stratify/With outliers/Adaptive_voronoi_testing_{test_filename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Comparisons.csv\", \"a\",encoding=\"UTF-8\") as csvfile:\n",
    "    csvwrite = csv.writer(csvfile)\n",
    "    csvwrite.writerow([\"Adaptive Voronoi\",\"Test\",epoch+1,optimizer_name,learning_rate,round(testing_accuracy*100,2),testing_loss.numpy(),[theta.numpy()],f\"{test_filename}.csv\",True,end_time-start_time])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
